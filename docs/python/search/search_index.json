{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"liburlparser for Python \u00b6 Fastest domain extractor library for Python Complete library for parsing URLs with Python and Command Line Overview \u00b6 liburlparser is a powerful domain extractor library written in C++ with Python bindings. It provides efficient URL parsing capabilities for Python, making it a valuable tool for projects that involve working with web addresses. Key Features \u00b6 High Performance : Significantly faster than pure Python alternatives Intuitive Interface : Simple, easy-to-use API Clean Code Design : Separate Url and Host classes for organized code Public Suffix List Support : Handles known combinatorial suffixes (e.g., \"ac.ir\") Unknown Suffix Support : Can handle unknown suffixes (e.g., \"comm\" in \"google.comm\") Automatic PSL Updates : Updates the public_suffix_list automatically Comprehensive Properties : Access all parts of URLs and hosts with simple property access Command Line Interface : Parse URLs directly from the command line Quick Start \u00b6 from liburlparser import Url, Host # Parse a URL url = Url(\"https://mail.google.com/about\") print(url.domain) # Output: google print(url.suffix) # Output: com print(url.protocol) # Output: https # Parse a host host = Host(\"mail.google.com\") print(host.subdomain) # Output: mail print(host.domain) # Output: google print(host.suffix) # Output: com Command Line \u00b6 python -m liburlparser --url \"https://mail.google.com/about\" | jq python -m liburlparser --host \"mail.google.com\" | jq Why liburlparser? \u00b6 Performance : Significantly faster than other domain extraction libraries Ease of Use : Simple, intuitive API Comprehensive : Handles all parts of URLs and hosts Reliable : Built on the Public Suffix List for accurate domain extraction Check out the Installation guide to get started, or dive into the Basic Usage documentation to learn more. Performance Comparison \u00b6 Extract From Host \u00b6 Tests were run on a file containing 10 million random domains from various top-level domains: Library Function Time liburlparser liburlparser.Host 1.12s PyDomainExtractor pydomainextractor.extract 1.50s publicsuffix2 publicsuffix2.get_sld 9.92s tldextract __call__ 29.23s tld tld.parse_tld 34.48s Extract From URL \u00b6 Tests were run on a file containing 1 million random URLs: Library Function Time liburlparser liburlparser.Host.from_url 2.10s PyDomainExtractor pydomainextractor.extract_from_url 2.24s publicsuffix2 publicsuffix2.get_sld 10.84s tldextract __call__ 36.04s tld tld.parse_tld 57.87s","title":"Home"},{"location":"#liburlparser-for-python","text":"","title":"liburlparser for Python"},{"location":"#overview","text":"liburlparser is a powerful domain extractor library written in C++ with Python bindings. It provides efficient URL parsing capabilities for Python, making it a valuable tool for projects that involve working with web addresses.","title":"Overview"},{"location":"#key-features","text":"High Performance : Significantly faster than pure Python alternatives Intuitive Interface : Simple, easy-to-use API Clean Code Design : Separate Url and Host classes for organized code Public Suffix List Support : Handles known combinatorial suffixes (e.g., \"ac.ir\") Unknown Suffix Support : Can handle unknown suffixes (e.g., \"comm\" in \"google.comm\") Automatic PSL Updates : Updates the public_suffix_list automatically Comprehensive Properties : Access all parts of URLs and hosts with simple property access Command Line Interface : Parse URLs directly from the command line","title":"Key Features"},{"location":"#quick-start","text":"from liburlparser import Url, Host # Parse a URL url = Url(\"https://mail.google.com/about\") print(url.domain) # Output: google print(url.suffix) # Output: com print(url.protocol) # Output: https # Parse a host host = Host(\"mail.google.com\") print(host.subdomain) # Output: mail print(host.domain) # Output: google print(host.suffix) # Output: com","title":"Quick Start"},{"location":"#command-line","text":"python -m liburlparser --url \"https://mail.google.com/about\" | jq python -m liburlparser --host \"mail.google.com\" | jq","title":"Command Line"},{"location":"#why-liburlparser","text":"Performance : Significantly faster than other domain extraction libraries Ease of Use : Simple, intuitive API Comprehensive : Handles all parts of URLs and hosts Reliable : Built on the Public Suffix List for accurate domain extraction Check out the Installation guide to get started, or dive into the Basic Usage documentation to learn more.","title":"Why liburlparser?"},{"location":"#performance-comparison","text":"","title":"Performance Comparison"},{"location":"#extract-from-host","text":"Tests were run on a file containing 10 million random domains from various top-level domains: Library Function Time liburlparser liburlparser.Host 1.12s PyDomainExtractor pydomainextractor.extract 1.50s publicsuffix2 publicsuffix2.get_sld 9.92s tldextract __call__ 29.23s tld tld.parse_tld 34.48s","title":"Extract From Host"},{"location":"#extract-from-url","text":"Tests were run on a file containing 1 million random URLs: Library Function Time liburlparser liburlparser.Host.from_url 2.10s PyDomainExtractor pydomainextractor.extract_from_url 2.24s publicsuffix2 publicsuffix2.get_sld 10.84s tldextract __call__ 36.04s tld tld.parse_tld 57.87s","title":"Extract From URL"},{"location":"examples/","text":"Examples \u00b6 This page provides practical examples of using liburlparser in Python for various URL parsing tasks. Basic URL Parsing \u00b6 from liburlparser import Url # Parse a simple URL url = Url(\"https://www.example.com\") print(f\"Domain: {url.domain}\") # Domain: example print(f\"Suffix: {url.suffix}\") # Suffix: com print(f\"Subdomain: {url.subdomain}\") # Subdomain: www # Parse a more complex URL url = Url(\"https://user:pass@mail.google.com:8080/path/to/page?q=test&lang=en#section\") print(f\"Protocol: {url.protocol}\") # Protocol: https print(f\"Userinfo: {url.userinfo}\") # Userinfo: user:pass print(f\"Domain: {url.domain}\") # Domain: google print(f\"Subdomain: {url.subdomain}\") # Subdomain: mail print(f\"Port: {url.port}\") # Port: 8080 print(f\"Query: {url.query}\") # Query: q=test&lang=en print(f\"Fragment: {url.fragment}\") # Fragment: section Host Parsing \u00b6 from liburlparser import Host # Parse a simple hostname host = Host(\"example.com\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: com print(f\"Subdomain: {host.subdomain}\") # Subdomain: (empty string) # Parse a hostname with subdomain host = Host(\"blog.example.co.uk\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: co.uk print(f\"Subdomain: {host.subdomain}\") # Subdomain: blog # Parse a hostname with multiple subdomain levels host = Host(\"a.b.c.example.com\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: com print(f\"Subdomain: {host.subdomain}\") # Subdomain: a.b.c Extracting Host from URL \u00b6 from liburlparser import Url, Host # Method 1: Get the host from a URL object url = Url(\"https://mail.google.com/about\") host = url.host print(f\"Host: {host}\") # Host: <Host :'mail.google.com'> # Method 2: Use the Host.from_url static method host = Host.from_url(\"https://mail.google.com/about\") print(f\"Domain: {host.domain}\") # Domain: google # Method 3: Just extract the host string (fastest) host_str = Url.extract_host(\"https://mail.google.com/about\") print(f\"Host string: {host_str}\") # Host string: mail.google.com Ignoring \"www\" Subdomain \u00b6 from liburlparser import Url, Host # Default behavior host = Host(\"www.example.com\") print(f\"Subdomain: '{host.subdomain}'\") # Subdomain: 'www' print(f\"Domain: {host.domain}\") # Domain: example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(f\"Subdomain: '{host.subdomain}'\") # Subdomain: '' print(f\"Domain: {host.domain}\") # Domain: example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(f\"Subdomain: '{url.subdomain}'\") # Subdomain: '' print(f\"Domain: {url.domain}\") # Domain: example Converting to Dictionary or JSON \u00b6 from liburlparser import Url, Host import json # URL to dictionary url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(json.dumps(url_dict, indent=2)) # Output: # { # \"str\": \"https://mail.google.com/about?q=test#section\", # \"protocol\": \"https\", # \"userinfo\": \"\", # \"host\": { # \"str\": \"mail.google.com\", # \"subdomain\": \"mail\", # \"domain\": \"google\", # \"domain_name\": \"google\", # \"suffix\": \"com\" # }, # \"port\": 0, # \"query\": \"q=test\", # \"fragment\": \"section\" # } # URL to JSON url_json = url.to_json() print(url_json) # Output: {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"} # Host to dictionary host = Host(\"mail.google.com\") host_dict = host.to_dict() print(json.dumps(host_dict, indent=2)) # Output: # { # \"str\": \"mail.google.com\", # \"subdomain\": \"mail\", # \"domain\": \"google\", # \"domain_name\": \"google\", # \"suffix\": \"com\" # } # Host to JSON host_json = host.to_json() print(host_json) # Output: {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"} Quick Domain Extraction \u00b6 from liburlparser import Host # From a host string result = Host.extract(\"mail.google.com\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} # From a URL string result = Host.extract_from_url(\"https://mail.google.com/about\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} Batch Processing URLs \u00b6 from liburlparser import Host import csv def extract_domains_from_csv(input_file, url_column, output_file): with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile: reader = csv.DictReader(infile) fieldnames = list(reader.fieldnames) + ['domain', 'suffix', 'subdomain'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: url = row[url_column] try: # Use the fast extract_from_url method domain_info = Host.extract_from_url(url) row['domain'] = domain_info['domain'] row['suffix'] = domain_info['suffix'] row['subdomain'] = domain_info['subdomain'] except Exception: row['domain'] = '' row['suffix'] = '' row['subdomain'] = '' writer.writerow(row) # Example usage # extract_domains_from_csv('urls.csv', 'url_column', 'output.csv') Error Handling \u00b6 from liburlparser import Url, Host def safe_parse_url(url_str): try: url = Url(url_str) return url except Exception as e: print(f\"Error parsing URL '{url_str}': {e}\") return None def safe_parse_host(host_str): try: host = Host(host_str) return host except Exception as e: print(f\"Error parsing host '{host_str}': {e}\") return None # Test with valid and invalid URLs urls = [ \"https://example.com\", \"invalid://example.com\", \"https://example.com:invalid\", \"not a url\" ] for url_str in urls: url = safe_parse_url(url_str) if url: print(f\"Successfully parsed: {url.domain}\") Working with International Domain Names (IDNs) \u00b6 from liburlparser import Url, Host # Parse an IDN url = Url(\"https://\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {url.domain}\") # Domain: \u4f8b\u5b50 print(f\"Suffix: {url.suffix}\") # Suffix: \u6d4b\u8bd5 host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {host.domain}\") # Domain: \u4f8b\u5b50 print(f\"Suffix: {host.suffix}\") # Suffix: \u6d4b\u8bd5","title":"Examples"},{"location":"examples/#examples","text":"This page provides practical examples of using liburlparser in Python for various URL parsing tasks.","title":"Examples"},{"location":"examples/#basic-url-parsing","text":"from liburlparser import Url # Parse a simple URL url = Url(\"https://www.example.com\") print(f\"Domain: {url.domain}\") # Domain: example print(f\"Suffix: {url.suffix}\") # Suffix: com print(f\"Subdomain: {url.subdomain}\") # Subdomain: www # Parse a more complex URL url = Url(\"https://user:pass@mail.google.com:8080/path/to/page?q=test&lang=en#section\") print(f\"Protocol: {url.protocol}\") # Protocol: https print(f\"Userinfo: {url.userinfo}\") # Userinfo: user:pass print(f\"Domain: {url.domain}\") # Domain: google print(f\"Subdomain: {url.subdomain}\") # Subdomain: mail print(f\"Port: {url.port}\") # Port: 8080 print(f\"Query: {url.query}\") # Query: q=test&lang=en print(f\"Fragment: {url.fragment}\") # Fragment: section","title":"Basic URL Parsing"},{"location":"examples/#host-parsing","text":"from liburlparser import Host # Parse a simple hostname host = Host(\"example.com\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: com print(f\"Subdomain: {host.subdomain}\") # Subdomain: (empty string) # Parse a hostname with subdomain host = Host(\"blog.example.co.uk\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: co.uk print(f\"Subdomain: {host.subdomain}\") # Subdomain: blog # Parse a hostname with multiple subdomain levels host = Host(\"a.b.c.example.com\") print(f\"Domain: {host.domain}\") # Domain: example print(f\"Suffix: {host.suffix}\") # Suffix: com print(f\"Subdomain: {host.subdomain}\") # Subdomain: a.b.c","title":"Host Parsing"},{"location":"examples/#extracting-host-from-url","text":"from liburlparser import Url, Host # Method 1: Get the host from a URL object url = Url(\"https://mail.google.com/about\") host = url.host print(f\"Host: {host}\") # Host: <Host :'mail.google.com'> # Method 2: Use the Host.from_url static method host = Host.from_url(\"https://mail.google.com/about\") print(f\"Domain: {host.domain}\") # Domain: google # Method 3: Just extract the host string (fastest) host_str = Url.extract_host(\"https://mail.google.com/about\") print(f\"Host string: {host_str}\") # Host string: mail.google.com","title":"Extracting Host from URL"},{"location":"examples/#ignoring-www-subdomain","text":"from liburlparser import Url, Host # Default behavior host = Host(\"www.example.com\") print(f\"Subdomain: '{host.subdomain}'\") # Subdomain: 'www' print(f\"Domain: {host.domain}\") # Domain: example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(f\"Subdomain: '{host.subdomain}'\") # Subdomain: '' print(f\"Domain: {host.domain}\") # Domain: example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(f\"Subdomain: '{url.subdomain}'\") # Subdomain: '' print(f\"Domain: {url.domain}\") # Domain: example","title":"Ignoring \"www\" Subdomain"},{"location":"examples/#converting-to-dictionary-or-json","text":"from liburlparser import Url, Host import json # URL to dictionary url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(json.dumps(url_dict, indent=2)) # Output: # { # \"str\": \"https://mail.google.com/about?q=test#section\", # \"protocol\": \"https\", # \"userinfo\": \"\", # \"host\": { # \"str\": \"mail.google.com\", # \"subdomain\": \"mail\", # \"domain\": \"google\", # \"domain_name\": \"google\", # \"suffix\": \"com\" # }, # \"port\": 0, # \"query\": \"q=test\", # \"fragment\": \"section\" # } # URL to JSON url_json = url.to_json() print(url_json) # Output: {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"} # Host to dictionary host = Host(\"mail.google.com\") host_dict = host.to_dict() print(json.dumps(host_dict, indent=2)) # Output: # { # \"str\": \"mail.google.com\", # \"subdomain\": \"mail\", # \"domain\": \"google\", # \"domain_name\": \"google\", # \"suffix\": \"com\" # } # Host to JSON host_json = host.to_json() print(host_json) # Output: {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}","title":"Converting to Dictionary or JSON"},{"location":"examples/#quick-domain-extraction","text":"from liburlparser import Host # From a host string result = Host.extract(\"mail.google.com\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} # From a URL string result = Host.extract_from_url(\"https://mail.google.com/about\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'}","title":"Quick Domain Extraction"},{"location":"examples/#batch-processing-urls","text":"from liburlparser import Host import csv def extract_domains_from_csv(input_file, url_column, output_file): with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile: reader = csv.DictReader(infile) fieldnames = list(reader.fieldnames) + ['domain', 'suffix', 'subdomain'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: url = row[url_column] try: # Use the fast extract_from_url method domain_info = Host.extract_from_url(url) row['domain'] = domain_info['domain'] row['suffix'] = domain_info['suffix'] row['subdomain'] = domain_info['subdomain'] except Exception: row['domain'] = '' row['suffix'] = '' row['subdomain'] = '' writer.writerow(row) # Example usage # extract_domains_from_csv('urls.csv', 'url_column', 'output.csv')","title":"Batch Processing URLs"},{"location":"examples/#error-handling","text":"from liburlparser import Url, Host def safe_parse_url(url_str): try: url = Url(url_str) return url except Exception as e: print(f\"Error parsing URL '{url_str}': {e}\") return None def safe_parse_host(host_str): try: host = Host(host_str) return host except Exception as e: print(f\"Error parsing host '{host_str}': {e}\") return None # Test with valid and invalid URLs urls = [ \"https://example.com\", \"invalid://example.com\", \"https://example.com:invalid\", \"not a url\" ] for url_str in urls: url = safe_parse_url(url_str) if url: print(f\"Successfully parsed: {url.domain}\")","title":"Error Handling"},{"location":"examples/#working-with-international-domain-names-idns","text":"from liburlparser import Url, Host # Parse an IDN url = Url(\"https://\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {url.domain}\") # Domain: \u4f8b\u5b50 print(f\"Suffix: {url.suffix}\") # Suffix: \u6d4b\u8bd5 host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {host.domain}\") # Domain: \u4f8b\u5b50 print(f\"Suffix: {host.suffix}\") # Suffix: \u6d4b\u8bd5","title":"Working with International Domain Names (IDNs)"},{"location":"installation/","text":"Installation \u00b6 liburlparser requires Python 3.8 or higher. Using pip (Recommended) \u00b6 Install from PyPI: pip install liburlparser If you want to use the psl.update feature to update the public suffix list, install the online version: pip install \"liburlparser[online]\" Install from GitHub \u00b6 pip install git+https://github.com/mohammadraziei/liburlparser Manual Installation \u00b6 git clone https://github.com/mohammadraziei/liburlparser pip install ./liburlparser Verifying Installation \u00b6 After installation, you can verify that liburlparser is working correctly: import liburlparser print(liburlparser.__version__) # Try parsing a URL from liburlparser import Url url = Url(\"https://example.com\") print(url.domain) # Should output: example Dependencies \u00b6 liburlparser has minimal dependencies: Python 3.8 or higher The online version additionally requires: - requests (for updating the Public Suffix List) Platform Support \u00b6 liburlparser works on: Windows macOS Linux Troubleshooting \u00b6 If you encounter any issues during installation: Make sure you have Python 3.8 or higher: bash python --version Ensure you have the latest pip: bash pip install --upgrade pip If you're on Windows and encounter build issues, you may need to install Visual C++ build tools: bash pip install --upgrade setuptools wheel If you're on Linux and encounter build issues, you may need to install development tools: ```bash # Ubuntu/Debian sudo apt-get install python3-dev build-essential # Fedora/RHEL/CentOS sudo dnf install python3-devel gcc ``` If you continue to experience issues, please open an issue on GitHub .","title":"Installation"},{"location":"installation/#installation","text":"liburlparser requires Python 3.8 or higher.","title":"Installation"},{"location":"installation/#using-pip-recommended","text":"Install from PyPI: pip install liburlparser If you want to use the psl.update feature to update the public suffix list, install the online version: pip install \"liburlparser[online]\"","title":"Using pip (Recommended)"},{"location":"installation/#install-from-github","text":"pip install git+https://github.com/mohammadraziei/liburlparser","title":"Install from GitHub"},{"location":"installation/#manual-installation","text":"git clone https://github.com/mohammadraziei/liburlparser pip install ./liburlparser","title":"Manual Installation"},{"location":"installation/#verifying-installation","text":"After installation, you can verify that liburlparser is working correctly: import liburlparser print(liburlparser.__version__) # Try parsing a URL from liburlparser import Url url = Url(\"https://example.com\") print(url.domain) # Should output: example","title":"Verifying Installation"},{"location":"installation/#dependencies","text":"liburlparser has minimal dependencies: Python 3.8 or higher The online version additionally requires: - requests (for updating the Public Suffix List)","title":"Dependencies"},{"location":"installation/#platform-support","text":"liburlparser works on: Windows macOS Linux","title":"Platform Support"},{"location":"installation/#troubleshooting","text":"If you encounter any issues during installation: Make sure you have Python 3.8 or higher: bash python --version Ensure you have the latest pip: bash pip install --upgrade pip If you're on Windows and encounter build issues, you may need to install Visual C++ build tools: bash pip install --upgrade setuptools wheel If you're on Linux and encounter build issues, you may need to install development tools: ```bash # Ubuntu/Debian sudo apt-get install python3-dev build-essential # Fedora/RHEL/CentOS sudo dnf install python3-devel gcc ``` If you continue to experience issues, please open an issue on GitHub .","title":"Troubleshooting"},{"location":"performance/","text":"Performance \u00b6 liburlparser is designed to be extremely fast and efficient. This page provides performance benchmarks and tips for optimizing your code when working with liburlparser. Benchmarks \u00b6 Extract From Host \u00b6 Tests were run on a file containing 10 million random domains from various top-level domains: Library Function Time liburlparser liburlparser.Host 1.12s PyDomainExtractor pydomainextractor.extract 1.50s publicsuffix2 publicsuffix2.get_sld 9.92s tldextract __call__ 29.23s tld tld.parse_tld 34.48s Extract From URL \u00b6 Tests were run on a file containing 1 million random URLs: Library Function Time liburlparser liburlparser.Host.from_url 2.10s PyDomainExtractor pydomainextractor.extract_from_url 2.24s publicsuffix2 publicsuffix2.get_sld 10.84s tldextract __call__ 36.04s tld tld.parse_tld 57.87s Performance Optimization Tips \u00b6 1. Choose the Right Method \u00b6 liburlparser provides several methods for extracting domain information, each with different performance characteristics: from liburlparser import Url, Host import time url_str = \"https://mail.google.com/about\" # Method 1: Full Url object creation (slowest, but provides all URL components) start = time.time() url = Url(url_str) domain = url.domain print(f\"Method 1 time: {time.time() - start:.6f}s\") # Method 2: Host object from URL (faster, provides all host components) start = time.time() host = Host.from_url(url_str) domain = host.domain print(f\"Method 2 time: {time.time() - start:.6f}s\") # Method 3: Extract host string only (very fast) start = time.time() host_str = Url.extract_host(url_str) print(f\"Method 3 time: {time.time() - start:.6f}s\") # Method 4: Extract components directly (fastest for domain extraction) start = time.time() components = Host.extract_from_url(url_str) domain = components[\"domain\"] print(f\"Method 4 time: {time.time() - start:.6f}s\") 2. Batch Processing \u00b6 For processing large numbers of URLs, use the fastest method appropriate for your needs: from liburlparser import Host import time # Sample list of URLs urls = [\"https://example.com\", \"https://google.com\", \"https://github.com\"] * 1000 # Method 1: Creating full Host objects start = time.time() domains1 = [] for url in urls: host = Host.from_url(url) domains1.append(host.domain) print(f\"Method 1 time: {time.time() - start:.4f}s\") # Method 2: Using extract_from_url (faster) start = time.time() domains2 = [] for url in urls: info = Host.extract_from_url(url) domains2.append(info[\"domain\"]) print(f\"Method 2 time: {time.time() - start:.4f}s\") 3. Memory Optimization \u00b6 If you're processing millions of URLs and memory usage is a concern, use the extraction methods instead of creating full objects: # Memory-efficient processing from liburlparser import Host def process_url_file(input_file, output_file): with open(input_file, 'r') as infile, open(output_file, 'w') as outfile: for line in infile: url = line.strip() try: # Use extract_from_url instead of creating Host objects info = Host.extract_from_url(url) outfile.write(f\"{url},{info['domain']},{info['suffix']}\\n\") except Exception: outfile.write(f\"{url},error,error\\n\") 4. Profiling Your Code \u00b6 You can use Python's built-in profiling tools to identify performance bottlenecks: import cProfile from liburlparser import Host def process_urls(urls): results = [] for url in urls: info = Host.extract_from_url(url) results.append(info[\"domain\"]) return results # Generate sample data urls = [\"https://example.com\", \"https://google.com\", \"https://github.com\"] * 1000 # Profile the function cProfile.run('process_urls(urls)') Comparison with Other Libraries \u00b6 liburlparser is significantly faster than other Python domain extraction libraries because: It's implemented in C++ with Python bindings It uses efficient data structures for the Public Suffix List It provides specialized methods for different use cases If you're migrating from another library, here's how liburlparser compares: # tldextract import tldextract extracted = tldextract.extract(\"mail.google.com\") domain = extracted.domain suffix = extracted.suffix subdomain = extracted.subdomain # liburlparser equivalent (much faster) from liburlparser import Host host = Host(\"mail.google.com\") domain = host.domain suffix = host.suffix subdomain = host.subdomain # publicsuffix2 from publicsuffix2 import get_sld domain = get_sld(\"mail.google.com\") # liburlparser equivalent from liburlparser import Host domain = Host(\"mail.google.com\").domain # pydomainextractor import pydomainextractor extractor = pydomainextractor.DomainExtractor() result = extractor.extract(\"mail.google.com\") # liburlparser equivalent from liburlparser import Host result = Host.extract(\"mail.google.com\")","title":"Performance"},{"location":"performance/#performance","text":"liburlparser is designed to be extremely fast and efficient. This page provides performance benchmarks and tips for optimizing your code when working with liburlparser.","title":"Performance"},{"location":"performance/#benchmarks","text":"","title":"Benchmarks"},{"location":"performance/#extract-from-host","text":"Tests were run on a file containing 10 million random domains from various top-level domains: Library Function Time liburlparser liburlparser.Host 1.12s PyDomainExtractor pydomainextractor.extract 1.50s publicsuffix2 publicsuffix2.get_sld 9.92s tldextract __call__ 29.23s tld tld.parse_tld 34.48s","title":"Extract From Host"},{"location":"performance/#extract-from-url","text":"Tests were run on a file containing 1 million random URLs: Library Function Time liburlparser liburlparser.Host.from_url 2.10s PyDomainExtractor pydomainextractor.extract_from_url 2.24s publicsuffix2 publicsuffix2.get_sld 10.84s tldextract __call__ 36.04s tld tld.parse_tld 57.87s","title":"Extract From URL"},{"location":"performance/#performance-optimization-tips","text":"","title":"Performance Optimization Tips"},{"location":"performance/#1-choose-the-right-method","text":"liburlparser provides several methods for extracting domain information, each with different performance characteristics: from liburlparser import Url, Host import time url_str = \"https://mail.google.com/about\" # Method 1: Full Url object creation (slowest, but provides all URL components) start = time.time() url = Url(url_str) domain = url.domain print(f\"Method 1 time: {time.time() - start:.6f}s\") # Method 2: Host object from URL (faster, provides all host components) start = time.time() host = Host.from_url(url_str) domain = host.domain print(f\"Method 2 time: {time.time() - start:.6f}s\") # Method 3: Extract host string only (very fast) start = time.time() host_str = Url.extract_host(url_str) print(f\"Method 3 time: {time.time() - start:.6f}s\") # Method 4: Extract components directly (fastest for domain extraction) start = time.time() components = Host.extract_from_url(url_str) domain = components[\"domain\"] print(f\"Method 4 time: {time.time() - start:.6f}s\")","title":"1. Choose the Right Method"},{"location":"performance/#2-batch-processing","text":"For processing large numbers of URLs, use the fastest method appropriate for your needs: from liburlparser import Host import time # Sample list of URLs urls = [\"https://example.com\", \"https://google.com\", \"https://github.com\"] * 1000 # Method 1: Creating full Host objects start = time.time() domains1 = [] for url in urls: host = Host.from_url(url) domains1.append(host.domain) print(f\"Method 1 time: {time.time() - start:.4f}s\") # Method 2: Using extract_from_url (faster) start = time.time() domains2 = [] for url in urls: info = Host.extract_from_url(url) domains2.append(info[\"domain\"]) print(f\"Method 2 time: {time.time() - start:.4f}s\")","title":"2. Batch Processing"},{"location":"performance/#3-memory-optimization","text":"If you're processing millions of URLs and memory usage is a concern, use the extraction methods instead of creating full objects: # Memory-efficient processing from liburlparser import Host def process_url_file(input_file, output_file): with open(input_file, 'r') as infile, open(output_file, 'w') as outfile: for line in infile: url = line.strip() try: # Use extract_from_url instead of creating Host objects info = Host.extract_from_url(url) outfile.write(f\"{url},{info['domain']},{info['suffix']}\\n\") except Exception: outfile.write(f\"{url},error,error\\n\")","title":"3. Memory Optimization"},{"location":"performance/#4-profiling-your-code","text":"You can use Python's built-in profiling tools to identify performance bottlenecks: import cProfile from liburlparser import Host def process_urls(urls): results = [] for url in urls: info = Host.extract_from_url(url) results.append(info[\"domain\"]) return results # Generate sample data urls = [\"https://example.com\", \"https://google.com\", \"https://github.com\"] * 1000 # Profile the function cProfile.run('process_urls(urls)')","title":"4. Profiling Your Code"},{"location":"performance/#comparison-with-other-libraries","text":"liburlparser is significantly faster than other Python domain extraction libraries because: It's implemented in C++ with Python bindings It uses efficient data structures for the Public Suffix List It provides specialized methods for different use cases If you're migrating from another library, here's how liburlparser compares: # tldextract import tldextract extracted = tldextract.extract(\"mail.google.com\") domain = extracted.domain suffix = extracted.suffix subdomain = extracted.subdomain # liburlparser equivalent (much faster) from liburlparser import Host host = Host(\"mail.google.com\") domain = host.domain suffix = host.suffix subdomain = host.subdomain # publicsuffix2 from publicsuffix2 import get_sld domain = get_sld(\"mail.google.com\") # liburlparser equivalent from liburlparser import Host domain = Host(\"mail.google.com\").domain # pydomainextractor import pydomainextractor extractor = pydomainextractor.DomainExtractor() result = extractor.extract(\"mail.google.com\") # liburlparser equivalent from liburlparser import Host result = Host.extract(\"mail.google.com\")","title":"Comparison with Other Libraries"},{"location":"api/host/","text":"Host Class API Reference \u00b6 The Host class represents a hostname and provides methods to parse and extract information from it. Import \u00b6 from liburlparser import Host Constructor \u00b6 Host(hoststr, ignore_www=False) Parameters \u00b6 hoststr (str): The hostname to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False . Example \u00b6 # Create a Host object host = Host(\"mail.google.com\") # Create a Host object, ignoring \"www\" if present host = Host(\"www.example.com\", ignore_www=True) Class Methods \u00b6 from_url \u00b6 @classmethod def from_url(cls, urlstr, ignore_www=False) Create a Host object from a URL string. Parameters \u00b6 urlstr (str): The URL string to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False . Returns \u00b6 A new Host object Example \u00b6 host = Host.from_url(\"https://mail.google.com/about\") print(host.domain) # google extract_from_url \u00b6 @classmethod def extract_from_url(cls, urlstr) Extract domain components from a URL string. Parameters \u00b6 urlstr (str): The URL string to parse Returns \u00b6 A dictionary with keys 'subdomain', 'domain', and 'suffix' Example \u00b6 info = Host.extract_from_url(\"https://mail.google.com/about\") print(info) # {'subdomain': 'mail', 'domain': 'google', 'suffix': 'com'} extract \u00b6 @classmethod def extract(cls, hoststr) Extract domain components from a host string. Parameters \u00b6 hoststr (str): The hostname to parse Returns \u00b6 A dictionary with keys 'subdomain', 'domain', and 'suffix' Example \u00b6 info = Host.extract(\"mail.google.com\") print(info) # {'subdomain': 'mail', 'domain': 'google', 'suffix': 'com'} load_psl_from_path \u00b6 @classmethod def load_psl_from_path(cls, filepath) Load the Public Suffix List from a file. Parameters \u00b6 filepath (str): Path to the PSL file Example \u00b6 Host.load_psl_from_path(\"/path/to/public_suffix_list.dat\") load_psl_from_string \u00b6 @classmethod def load_psl_from_string(cls, string) Load the Public Suffix List from a string. Parameters \u00b6 string (str): The PSL content as a string Example \u00b6 with open(\"/path/to/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() Host.load_psl_from_string(psl_content) is_psl_loaded \u00b6 @classmethod def is_psl_loaded(cls) Check if the Public Suffix List is loaded. Returns \u00b6 True if loaded, False otherwise Example \u00b6 is_loaded = Host.is_psl_loaded() print(f\"PSL loaded: {is_loaded}\") removeWWW \u00b6 @classmethod def removeWWW(cls, hoststr) Remove \"www.\" from the beginning of a hostname. Parameters \u00b6 hoststr (str): The hostname Returns \u00b6 The hostname without \"www.\" Example \u00b6 host_without_www = Host.removeWWW(\"www.example.com\") print(host_without_www) # example.com Properties \u00b6 subdomain \u00b6 The subdomain part of the hostname. host = Host(\"mail.google.com\") print(host.subdomain) # mail domain \u00b6 The domain part of the hostname. host = Host(\"mail.google.com\") print(host.domain) # google domain_name \u00b6 The domain name (same as domain). host = Host(\"mail.google.com\") print(host.domain_name) # google fulldomain \u00b6 The full domain (domain + suffix). host = Host(\"mail.google.com\") print(host.fulldomain) # google.com suffix \u00b6 The suffix part of the hostname. host = Host(\"mail.google.com\") print(host.suffix) # com Methods \u00b6 to_dict \u00b6 Convert the Host object to a dictionary. host.to_dict() Returns \u00b6 A dictionary with keys 'str', 'subdomain', 'domain', 'domain_name', and 'suffix' Example \u00b6 host = Host(\"mail.google.com\") host_dict = host.to_dict() print(host_dict) # {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'} to_json \u00b6 Convert the Host object to a JSON string. host = Host(\"mail.google.com\") host_json = host.to_json() print(host_json) # {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"} str \u00b6 Get the string representation of the hostname. str(host) Returns \u00b6 The hostname as a string Example \u00b6 host = Host(\"mail.google.com\") print(str(host)) # mail.google.com repr \u00b6 Get the representation of the Host object. repr(host) Returns \u00b6 A string like <Host :'example.com'> Example \u00b6 host = Host(\"mail.google.com\") print(repr(host)) # <Host :'mail.google.com'> Complete Example \u00b6 Here's a complete example that demonstrates the Host class functionality: from liburlparser import Host def analyze_host(host_str): # Parse the host host = Host(host_str) # Print host components print(f\"Full host: {host}\") print(f\"Subdomain: {host.subdomain}\") print(f\"Domain: {host.domain}\") print(f\"Suffix: {host.suffix}\") print(f\"Full domain: {host.fulldomain}\") # Convert to dictionary and JSON print(f\"Dictionary: {host.to_dict()}\") print(f\"JSON: {host.to_json()}\") # Test with different hostnames hosts = [ \"example.com\", \"www.example.com\", \"mail.google.com\", \"blog.example.co.uk\", \"a.b.c.example.org\" ] for host_str in hosts: print(f\"\\nAnalyzing: {host_str}\") analyze_host(host_str) Handling Complex Domains \u00b6 The Host class can handle various domain structures: # Standard domains host = Host(\"example.com\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: com # Domains with subdomains host = Host(\"www.mail.example.com\") print(f\"Subdomain: {host.subdomain}, Domain: {host.domain}, Suffix: {host.suffix}\") # Subdomain: www.mail, Domain: example, Suffix: com # Country-specific domains host = Host(\"example.co.uk\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: co.uk # Domains with unusual TLDs host = Host(\"example.museum\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: museum # IDN domains host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: \u4f8b\u5b50, Suffix: \u6d4b\u8bd5 Error Handling \u00b6 It's good practice to handle potential errors when parsing hosts: def safe_parse_host(host_str): try: host = Host(host_str) return { 'success': True, 'host': host, 'domain': host.domain, 'suffix': host.suffix, 'subdomain': host.subdomain } except Exception as e: return { 'success': False, 'error': str(e), 'host_str': host_str } # Test with valid and invalid hosts hosts = [ \"example.com\", \"mail.google.com\", \"\", # Empty string \"invalid\" # No suffix ] for host_str in hosts: result = safe_parse_host(host_str) if result['success']: print(f\"Successfully parsed: {host_str} \u2192 Domain: {result['domain']}\") else: print(f\"Failed to parse: {host_str} \u2192 Error: {result['error']}\")","title":"Host"},{"location":"api/host/#host-class-api-reference","text":"The Host class represents a hostname and provides methods to parse and extract information from it.","title":"Host Class API Reference"},{"location":"api/host/#import","text":"from liburlparser import Host","title":"Import"},{"location":"api/host/#constructor","text":"Host(hoststr, ignore_www=False)","title":"Constructor"},{"location":"api/host/#parameters","text":"hoststr (str): The hostname to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False .","title":"Parameters"},{"location":"api/host/#example","text":"# Create a Host object host = Host(\"mail.google.com\") # Create a Host object, ignoring \"www\" if present host = Host(\"www.example.com\", ignore_www=True)","title":"Example"},{"location":"api/host/#class-methods","text":"","title":"Class Methods"},{"location":"api/host/#from_url","text":"@classmethod def from_url(cls, urlstr, ignore_www=False) Create a Host object from a URL string.","title":"from_url"},{"location":"api/host/#parameters_1","text":"urlstr (str): The URL string to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False .","title":"Parameters"},{"location":"api/host/#returns","text":"A new Host object","title":"Returns"},{"location":"api/host/#example_1","text":"host = Host.from_url(\"https://mail.google.com/about\") print(host.domain) # google","title":"Example"},{"location":"api/host/#extract_from_url","text":"@classmethod def extract_from_url(cls, urlstr) Extract domain components from a URL string.","title":"extract_from_url"},{"location":"api/host/#parameters_2","text":"urlstr (str): The URL string to parse","title":"Parameters"},{"location":"api/host/#returns_1","text":"A dictionary with keys 'subdomain', 'domain', and 'suffix'","title":"Returns"},{"location":"api/host/#example_2","text":"info = Host.extract_from_url(\"https://mail.google.com/about\") print(info) # {'subdomain': 'mail', 'domain': 'google', 'suffix': 'com'}","title":"Example"},{"location":"api/host/#extract","text":"@classmethod def extract(cls, hoststr) Extract domain components from a host string.","title":"extract"},{"location":"api/host/#parameters_3","text":"hoststr (str): The hostname to parse","title":"Parameters"},{"location":"api/host/#returns_2","text":"A dictionary with keys 'subdomain', 'domain', and 'suffix'","title":"Returns"},{"location":"api/host/#example_3","text":"info = Host.extract(\"mail.google.com\") print(info) # {'subdomain': 'mail', 'domain': 'google', 'suffix': 'com'}","title":"Example"},{"location":"api/host/#load_psl_from_path","text":"@classmethod def load_psl_from_path(cls, filepath) Load the Public Suffix List from a file.","title":"load_psl_from_path"},{"location":"api/host/#parameters_4","text":"filepath (str): Path to the PSL file","title":"Parameters"},{"location":"api/host/#example_4","text":"Host.load_psl_from_path(\"/path/to/public_suffix_list.dat\")","title":"Example"},{"location":"api/host/#load_psl_from_string","text":"@classmethod def load_psl_from_string(cls, string) Load the Public Suffix List from a string.","title":"load_psl_from_string"},{"location":"api/host/#parameters_5","text":"string (str): The PSL content as a string","title":"Parameters"},{"location":"api/host/#example_5","text":"with open(\"/path/to/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() Host.load_psl_from_string(psl_content)","title":"Example"},{"location":"api/host/#is_psl_loaded","text":"@classmethod def is_psl_loaded(cls) Check if the Public Suffix List is loaded.","title":"is_psl_loaded"},{"location":"api/host/#returns_3","text":"True if loaded, False otherwise","title":"Returns"},{"location":"api/host/#example_6","text":"is_loaded = Host.is_psl_loaded() print(f\"PSL loaded: {is_loaded}\")","title":"Example"},{"location":"api/host/#removewww","text":"@classmethod def removeWWW(cls, hoststr) Remove \"www.\" from the beginning of a hostname.","title":"removeWWW"},{"location":"api/host/#parameters_6","text":"hoststr (str): The hostname","title":"Parameters"},{"location":"api/host/#returns_4","text":"The hostname without \"www.\"","title":"Returns"},{"location":"api/host/#example_7","text":"host_without_www = Host.removeWWW(\"www.example.com\") print(host_without_www) # example.com","title":"Example"},{"location":"api/host/#properties","text":"","title":"Properties"},{"location":"api/host/#subdomain","text":"The subdomain part of the hostname. host = Host(\"mail.google.com\") print(host.subdomain) # mail","title":"subdomain"},{"location":"api/host/#domain","text":"The domain part of the hostname. host = Host(\"mail.google.com\") print(host.domain) # google","title":"domain"},{"location":"api/host/#domain_name","text":"The domain name (same as domain). host = Host(\"mail.google.com\") print(host.domain_name) # google","title":"domain_name"},{"location":"api/host/#fulldomain","text":"The full domain (domain + suffix). host = Host(\"mail.google.com\") print(host.fulldomain) # google.com","title":"fulldomain"},{"location":"api/host/#suffix","text":"The suffix part of the hostname. host = Host(\"mail.google.com\") print(host.suffix) # com","title":"suffix"},{"location":"api/host/#methods","text":"","title":"Methods"},{"location":"api/host/#to_dict","text":"Convert the Host object to a dictionary. host.to_dict()","title":"to_dict"},{"location":"api/host/#returns_5","text":"A dictionary with keys 'str', 'subdomain', 'domain', 'domain_name', and 'suffix'","title":"Returns"},{"location":"api/host/#example_8","text":"host = Host(\"mail.google.com\") host_dict = host.to_dict() print(host_dict) # {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'}","title":"Example"},{"location":"api/host/#to_json","text":"Convert the Host object to a JSON string. host = Host(\"mail.google.com\") host_json = host.to_json() print(host_json) # {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}","title":"to_json"},{"location":"api/host/#str","text":"Get the string representation of the hostname. str(host)","title":"str"},{"location":"api/host/#returns_6","text":"The hostname as a string","title":"Returns"},{"location":"api/host/#example_9","text":"host = Host(\"mail.google.com\") print(str(host)) # mail.google.com","title":"Example"},{"location":"api/host/#repr","text":"Get the representation of the Host object. repr(host)","title":"repr"},{"location":"api/host/#returns_7","text":"A string like <Host :'example.com'>","title":"Returns"},{"location":"api/host/#example_10","text":"host = Host(\"mail.google.com\") print(repr(host)) # <Host :'mail.google.com'>","title":"Example"},{"location":"api/host/#complete-example","text":"Here's a complete example that demonstrates the Host class functionality: from liburlparser import Host def analyze_host(host_str): # Parse the host host = Host(host_str) # Print host components print(f\"Full host: {host}\") print(f\"Subdomain: {host.subdomain}\") print(f\"Domain: {host.domain}\") print(f\"Suffix: {host.suffix}\") print(f\"Full domain: {host.fulldomain}\") # Convert to dictionary and JSON print(f\"Dictionary: {host.to_dict()}\") print(f\"JSON: {host.to_json()}\") # Test with different hostnames hosts = [ \"example.com\", \"www.example.com\", \"mail.google.com\", \"blog.example.co.uk\", \"a.b.c.example.org\" ] for host_str in hosts: print(f\"\\nAnalyzing: {host_str}\") analyze_host(host_str)","title":"Complete Example"},{"location":"api/host/#handling-complex-domains","text":"The Host class can handle various domain structures: # Standard domains host = Host(\"example.com\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: com # Domains with subdomains host = Host(\"www.mail.example.com\") print(f\"Subdomain: {host.subdomain}, Domain: {host.domain}, Suffix: {host.suffix}\") # Subdomain: www.mail, Domain: example, Suffix: com # Country-specific domains host = Host(\"example.co.uk\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: co.uk # Domains with unusual TLDs host = Host(\"example.museum\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: example, Suffix: museum # IDN domains host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(f\"Domain: {host.domain}, Suffix: {host.suffix}\") # Domain: \u4f8b\u5b50, Suffix: \u6d4b\u8bd5","title":"Handling Complex Domains"},{"location":"api/host/#error-handling","text":"It's good practice to handle potential errors when parsing hosts: def safe_parse_host(host_str): try: host = Host(host_str) return { 'success': True, 'host': host, 'domain': host.domain, 'suffix': host.suffix, 'subdomain': host.subdomain } except Exception as e: return { 'success': False, 'error': str(e), 'host_str': host_str } # Test with valid and invalid hosts hosts = [ \"example.com\", \"mail.google.com\", \"\", # Empty string \"invalid\" # No suffix ] for host_str in hosts: result = safe_parse_host(host_str) if result['success']: print(f\"Successfully parsed: {host_str} \u2192 Domain: {result['domain']}\") else: print(f\"Failed to parse: {host_str} \u2192 Error: {result['error']}\")","title":"Error Handling"},{"location":"api/psl/","text":"PSL API Reference \u00b6 The Public Suffix List (PSL) functionality in liburlparser is accessed through the global psl object. Overview \u00b6 The Public Suffix List is a list of all known public suffixes, such as \".com\", \".co.uk\", etc. liburlparser uses this list to accurately parse domain names and determine the correct domain and suffix parts. Global Object \u00b6 from liburlparser import psl The psl object is a global instance of the Psl class that is automatically created when you import liburlparser. Properties \u00b6 url \u00b6 The URL of the Public Suffix List. from liburlparser import psl print(psl.url) # https://publicsuffix.org/list/public_suffix_list.dat filename \u00b6 The filename of the Public Suffix List. from liburlparser import psl print(psl.filename) # public_suffix_list.dat Methods \u00b6 is_loaded \u00b6 Check if the Public Suffix List is loaded. from liburlparser import psl is_loaded = psl.is_loaded() print(f\"PSL loaded: {is_loaded}\") # PSL loaded: True load_from_path \u00b6 Load the Public Suffix List from a file. from liburlparser import psl psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\") load_from_string \u00b6 Load the Public Suffix List from a string. from liburlparser import psl with open(\"/path/to/custom/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() psl.load_from_string(psl_content) Example Usage \u00b6 from liburlparser import psl # Check if PSL is loaded if psl.is_loaded(): print(f\"PSL is loaded from {psl.filename}\") else: print(\"PSL is not loaded, loading from custom path...\") psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\") # Print PSL information print(f\"PSL URL: {psl.url}\") print(f\"PSL Filename: {psl.filename}\") Updating the PSL (Online Version Only) \u00b6 If you installed the online version ( pip install \"liburlparser[online]\" ), you can update the PSL: # This feature is only available if you installed with: # pip install \"liburlparser[online]\" from liburlparser import psl_updater # Update the PSL psl_updater.update()","title":"PSL"},{"location":"api/psl/#psl-api-reference","text":"The Public Suffix List (PSL) functionality in liburlparser is accessed through the global psl object.","title":"PSL API Reference"},{"location":"api/psl/#overview","text":"The Public Suffix List is a list of all known public suffixes, such as \".com\", \".co.uk\", etc. liburlparser uses this list to accurately parse domain names and determine the correct domain and suffix parts.","title":"Overview"},{"location":"api/psl/#global-object","text":"from liburlparser import psl The psl object is a global instance of the Psl class that is automatically created when you import liburlparser.","title":"Global Object"},{"location":"api/psl/#properties","text":"","title":"Properties"},{"location":"api/psl/#url","text":"The URL of the Public Suffix List. from liburlparser import psl print(psl.url) # https://publicsuffix.org/list/public_suffix_list.dat","title":"url"},{"location":"api/psl/#filename","text":"The filename of the Public Suffix List. from liburlparser import psl print(psl.filename) # public_suffix_list.dat","title":"filename"},{"location":"api/psl/#methods","text":"","title":"Methods"},{"location":"api/psl/#is_loaded","text":"Check if the Public Suffix List is loaded. from liburlparser import psl is_loaded = psl.is_loaded() print(f\"PSL loaded: {is_loaded}\") # PSL loaded: True","title":"is_loaded"},{"location":"api/psl/#load_from_path","text":"Load the Public Suffix List from a file. from liburlparser import psl psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\")","title":"load_from_path"},{"location":"api/psl/#load_from_string","text":"Load the Public Suffix List from a string. from liburlparser import psl with open(\"/path/to/custom/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() psl.load_from_string(psl_content)","title":"load_from_string"},{"location":"api/psl/#example-usage","text":"from liburlparser import psl # Check if PSL is loaded if psl.is_loaded(): print(f\"PSL is loaded from {psl.filename}\") else: print(\"PSL is not loaded, loading from custom path...\") psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\") # Print PSL information print(f\"PSL URL: {psl.url}\") print(f\"PSL Filename: {psl.filename}\")","title":"Example Usage"},{"location":"api/psl/#updating-the-psl-online-version-only","text":"If you installed the online version ( pip install \"liburlparser[online]\" ), you can update the PSL: # This feature is only available if you installed with: # pip install \"liburlparser[online]\" from liburlparser import psl_updater # Update the PSL psl_updater.update()","title":"Updating the PSL (Online Version Only)"},{"location":"api/url/","text":"Url Class API Reference \u00b6 The Url class is used to parse and extract information from URLs. Import \u00b6 from liburlparser import Url Constructor \u00b6 Url(urlstr, ignore_www=False) Parameters \u00b6 urlstr (str): The URL string to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False . Example \u00b6 # Create a Url object url = Url(\"https://mail.google.com/about\") # Create a Url object, ignoring \"www\" if present url = Url(\"https://www.example.com/about\", ignore_www=True) Class Methods \u00b6 extract_host \u00b6 @classmethod def extract_host(cls, urlstr) Extract the host part from a URL string. Parameters \u00b6 urlstr (str): The URL string to parse Returns \u00b6 The hostname as a string Example \u00b6 host_str = Url.extract_host(\"https://mail.google.com/about\") print(host_str) # mail.google.com Properties \u00b6 protocol \u00b6 The protocol part of the URL (e.g., \"http\", \"https\"). url = Url(\"https://mail.google.com/about\") print(url.protocol) # https userinfo \u00b6 The userinfo part of the URL (username and password). url = Url(\"https://user:pass@example.com\") print(url.userinfo) # user:pass host \u00b6 The host part of the URL as a Host object. url = Url(\"https://mail.google.com/about\") print(url.host) # <Host :'mail.google.com'> subdomain \u00b6 The subdomain part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.subdomain) # mail domain \u00b6 The domain part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.domain) # google fulldomain \u00b6 The full domain (domain + suffix). url = Url(\"https://mail.google.com/about\") print(url.fulldomain) # google.com domain_name \u00b6 The domain name (same as domain). url = Url(\"https://mail.google.com/about\") print(url.domain_name) # google suffix \u00b6 The suffix part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.suffix) # com port \u00b6 The port number (0 if not specified). url = Url(\"https://example.com:8080\") print(url.port) # 8080 url = Url(\"https://example.com\") print(url.port) # 0 params \u00b6 The parameters part of the URL. url = Url(\"https://example.com/path;param1=value1;param2=value2\") print(url.params) # param1=value1;param2=value2 query \u00b6 The query part of the URL. url = Url(\"https://example.com/path?q=test&page=1\") print(url.query) # q=test&page=1 fragment \u00b6 The fragment part of the URL. url = Url(\"https://example.com/path#section\") print(url.fragment) # section Methods \u00b6 to_dict \u00b6 Convert the Url object to a dictionary. url.to_dict() Returns \u00b6 A dictionary with keys 'str', 'protocol', 'userinfo', 'host', 'port', 'query', and 'fragment' Example \u00b6 url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(url_dict) # {'str': 'https://mail.google.com/about?q=test#section', 'protocol': 'https', 'userinfo': '', 'host': {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'}, 'port': 0, 'query': 'q=test', 'fragment': 'section'} to_json \u00b6 Convert the Url object to a JSON string. url.to_json() Returns \u00b6 A JSON string representation of the Url object Example \u00b6 url = Url(\"https://mail.google.com/about?q=test#section\") url_json = url.to_json() print(url_json) # {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"} str \u00b6 Get the string representation of the URL. str(url) Returns \u00b6 The URL as a string Example \u00b6 url = Url(\"https://mail.google.com/about\") print(str(url)) # https://mail.google.com/about repr \u00b6 Get the representation of the Url object. repr(url) Returns \u00b6 A string like <Url :'https://example.com'> Example \u00b6 url = Url(\"https://mail.google.com/about\") print(repr(url)) # <Url :'https://mail.google.com/about'> Complete Example \u00b6 from liburlparser import Url # Parse a complex URL url = Url(\"https://user:pass@sub.example.co.uk:8080/path/to/page?q=test&lang=en#section\") # Access all properties print(f\"Full URL: {url}\") print(f\"Protocol: {url.protocol}\") print(f\"Userinfo: {url.userinfo}\") print(f\"Host: {url.host}\") print(f\"Subdomain: {url.subdomain}\") print(f\"Domain: {url.domain}\") print(f\"Suffix: {url.suffix}\") print(f\"Full Domain: {url.fulldomain}\") print(f\"Port: {url.port}\") print(f\"Query: {url.query}\") print(f\"Fragment: {url.fragment}\") # Convert to dictionary and JSON print(f\"Dictionary: {url.to_dict()}\") print(f\"JSON: {url.to_json()}\")","title":"Url"},{"location":"api/url/#url-class-api-reference","text":"The Url class is used to parse and extract information from URLs.","title":"Url Class API Reference"},{"location":"api/url/#import","text":"from liburlparser import Url","title":"Import"},{"location":"api/url/#constructor","text":"Url(urlstr, ignore_www=False)","title":"Constructor"},{"location":"api/url/#parameters","text":"urlstr (str): The URL string to parse ignore_www (bool, optional): Whether to ignore \"www\" in the subdomain. Default is False .","title":"Parameters"},{"location":"api/url/#example","text":"# Create a Url object url = Url(\"https://mail.google.com/about\") # Create a Url object, ignoring \"www\" if present url = Url(\"https://www.example.com/about\", ignore_www=True)","title":"Example"},{"location":"api/url/#class-methods","text":"","title":"Class Methods"},{"location":"api/url/#extract_host","text":"@classmethod def extract_host(cls, urlstr) Extract the host part from a URL string.","title":"extract_host"},{"location":"api/url/#parameters_1","text":"urlstr (str): The URL string to parse","title":"Parameters"},{"location":"api/url/#returns","text":"The hostname as a string","title":"Returns"},{"location":"api/url/#example_1","text":"host_str = Url.extract_host(\"https://mail.google.com/about\") print(host_str) # mail.google.com","title":"Example"},{"location":"api/url/#properties","text":"","title":"Properties"},{"location":"api/url/#protocol","text":"The protocol part of the URL (e.g., \"http\", \"https\"). url = Url(\"https://mail.google.com/about\") print(url.protocol) # https","title":"protocol"},{"location":"api/url/#userinfo","text":"The userinfo part of the URL (username and password). url = Url(\"https://user:pass@example.com\") print(url.userinfo) # user:pass","title":"userinfo"},{"location":"api/url/#host","text":"The host part of the URL as a Host object. url = Url(\"https://mail.google.com/about\") print(url.host) # <Host :'mail.google.com'>","title":"host"},{"location":"api/url/#subdomain","text":"The subdomain part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.subdomain) # mail","title":"subdomain"},{"location":"api/url/#domain","text":"The domain part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.domain) # google","title":"domain"},{"location":"api/url/#fulldomain","text":"The full domain (domain + suffix). url = Url(\"https://mail.google.com/about\") print(url.fulldomain) # google.com","title":"fulldomain"},{"location":"api/url/#domain_name","text":"The domain name (same as domain). url = Url(\"https://mail.google.com/about\") print(url.domain_name) # google","title":"domain_name"},{"location":"api/url/#suffix","text":"The suffix part of the hostname. url = Url(\"https://mail.google.com/about\") print(url.suffix) # com","title":"suffix"},{"location":"api/url/#port","text":"The port number (0 if not specified). url = Url(\"https://example.com:8080\") print(url.port) # 8080 url = Url(\"https://example.com\") print(url.port) # 0","title":"port"},{"location":"api/url/#params","text":"The parameters part of the URL. url = Url(\"https://example.com/path;param1=value1;param2=value2\") print(url.params) # param1=value1;param2=value2","title":"params"},{"location":"api/url/#query","text":"The query part of the URL. url = Url(\"https://example.com/path?q=test&page=1\") print(url.query) # q=test&page=1","title":"query"},{"location":"api/url/#fragment","text":"The fragment part of the URL. url = Url(\"https://example.com/path#section\") print(url.fragment) # section","title":"fragment"},{"location":"api/url/#methods","text":"","title":"Methods"},{"location":"api/url/#to_dict","text":"Convert the Url object to a dictionary. url.to_dict()","title":"to_dict"},{"location":"api/url/#returns_1","text":"A dictionary with keys 'str', 'protocol', 'userinfo', 'host', 'port', 'query', and 'fragment'","title":"Returns"},{"location":"api/url/#example_2","text":"url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(url_dict) # {'str': 'https://mail.google.com/about?q=test#section', 'protocol': 'https', 'userinfo': '', 'host': {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'}, 'port': 0, 'query': 'q=test', 'fragment': 'section'}","title":"Example"},{"location":"api/url/#to_json","text":"Convert the Url object to a JSON string. url.to_json()","title":"to_json"},{"location":"api/url/#returns_2","text":"A JSON string representation of the Url object","title":"Returns"},{"location":"api/url/#example_3","text":"url = Url(\"https://mail.google.com/about?q=test#section\") url_json = url.to_json() print(url_json) # {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"}","title":"Example"},{"location":"api/url/#str","text":"Get the string representation of the URL. str(url)","title":"str"},{"location":"api/url/#returns_3","text":"The URL as a string","title":"Returns"},{"location":"api/url/#example_4","text":"url = Url(\"https://mail.google.com/about\") print(str(url)) # https://mail.google.com/about","title":"Example"},{"location":"api/url/#repr","text":"Get the representation of the Url object. repr(url)","title":"repr"},{"location":"api/url/#returns_4","text":"A string like <Url :'https://example.com'>","title":"Returns"},{"location":"api/url/#example_5","text":"url = Url(\"https://mail.google.com/about\") print(repr(url)) # <Url :'https://mail.google.com/about'>","title":"Example"},{"location":"api/url/#complete-example","text":"from liburlparser import Url # Parse a complex URL url = Url(\"https://user:pass@sub.example.co.uk:8080/path/to/page?q=test&lang=en#section\") # Access all properties print(f\"Full URL: {url}\") print(f\"Protocol: {url.protocol}\") print(f\"Userinfo: {url.userinfo}\") print(f\"Host: {url.host}\") print(f\"Subdomain: {url.subdomain}\") print(f\"Domain: {url.domain}\") print(f\"Suffix: {url.suffix}\") print(f\"Full Domain: {url.fulldomain}\") print(f\"Port: {url.port}\") print(f\"Query: {url.query}\") print(f\"Fragment: {url.fragment}\") # Convert to dictionary and JSON print(f\"Dictionary: {url.to_dict()}\") print(f\"JSON: {url.to_json()}\")","title":"Complete Example"},{"location":"usage/advanced/","text":"Advanced Usage \u00b6 This guide covers more advanced features and techniques for using liburlparser in Python. Working with the Public Suffix List (PSL) \u00b6 liburlparser uses the Public Suffix List to accurately parse domain suffixes. You can interact with the PSL through the global psl object: from liburlparser import psl # Check if PSL is loaded print(psl.is_loaded()) # True (by default) # Get PSL information print(psl.url) # URL of the public suffix list print(psl.filename) # Filename of the public suffix list # Load PSL from a custom path psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\") # Load PSL from a string with open(\"/path/to/custom/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() psl.load_from_string(psl_content) Ignoring \"www\" Subdomain \u00b6 You can choose to ignore the \"www\" subdomain when parsing: # Default behavior host = Host(\"www.example.com\") print(host.subdomain) # www print(host.domain) # example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(host.subdomain) # (empty string) print(host.domain) # example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(url.subdomain) # (empty string) print(url.domain) # example Removing \"www\" from a Host String \u00b6 If you just need to remove \"www.\" from a hostname: from liburlparser import Host host_str = Host.removeWWW(\"www.example.com\") print(host_str) # example.com Handling Complex Domains \u00b6 liburlparser can handle complex domain structures, including multi-part suffixes: # Standard TLDs host = Host(\"example.com\") print(host.domain) # example print(host.suffix) # com # Country-specific TLDs host = Host(\"example.co.uk\") print(host.domain) # example print(host.suffix) # co.uk # Multi-level subdomains host = Host(\"a.b.c.example.com\") print(host.subdomain) # a.b.c print(host.domain) # example print(host.suffix) # com Compatibility with Other Libraries \u00b6 If you're migrating from pydomainextractor, liburlparser provides compatible methods: # pydomainextractor style: import pydomainextractor extractor = pydomainextractor.DomainExtractor() result = extractor.extract(\"mail.google.com\") result = extractor.extract_from_url(\"https://mail.google.com/about\") # liburlparser equivalent: from liburlparser import Host result = Host.extract(\"mail.google.com\") result = Host.extract_from_url(\"https://mail.google.com/about\") Error Handling \u00b6 liburlparser is designed to be robust and handle various edge cases: # Empty or invalid URLs try: url = Url(\"\") except Exception as e: print(f\"Error: {e}\") # URLs without protocols url = Url(\"example.com\") print(url.protocol) # (empty string) print(url.domain) # example # URLs with unusual formats url = Url(\"https://user:pass@example.com:8080/path?query=value#fragment\") print(url.userinfo) # user:pass print(url.port) # 8080 print(url.query) # query=value Performance Optimization \u00b6 For high-performance applications, consider these techniques: Use Host.from_url() instead of creating a Url object when you only need host information: # Slower (creates full Url object) url = Url(\"https://example.com/path\") host = url.host # Faster (directly extracts host) host = Host.from_url(\"https://example.com/path\") Use Url.extract_host() when you only need the host string: # Fastest (just extracts host string without parsing) host_str = Url.extract_host(\"https://example.com/path\") Use Host.extract() or Host.extract_from_url() when you only need domain components: # Fast (returns just the components you need) components = Host.extract(\"example.com\") components = Host.extract_from_url(\"https://example.com/path\") Working with International Domain Names (IDNs) \u00b6 liburlparser has built-in support for International Domain Names (IDNs): # Parse IDNs directly host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(host.domain) # \u4f8b\u5b50 print(host.suffix) # \u6d4b\u8bd5 # IDNs in URLs url = Url(\"https://\u4f8b\u5b50.\u6d4b\u8bd5/path\") print(url.domain) # \u4f8b\u5b50 print(url.suffix) # \u6d4b\u8bd5 # Mixed IDN and ASCII host = Host(\"subdomain.\u4f8b\u5b50.\u6d4b\u8bd5\") print(host.subdomain) # subdomain print(host.domain) # \u4f8b\u5b50 print(host.suffix) # \u6d4b\u8bd5 Batch Processing \u00b6 For processing large numbers of URLs efficiently: from liburlparser import Host import csv def extract_domains_from_csv(csv_file, url_column, output_file): with open(csv_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile: reader = csv.DictReader(infile) fieldnames = list(reader.fieldnames) + ['domain', 'suffix', 'subdomain'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: url = row[url_column] try: # Use the fast extract_from_url method domain_info = Host.extract_from_url(url) row['domain'] = domain_info['domain'] row['suffix'] = domain_info['suffix'] row['subdomain'] = domain_info['subdomain'] except Exception: row['domain'] = '' row['suffix'] = '' row['subdomain'] = '' writer.writerow(row) # Example usage # extract_domains_from_csv('urls.csv', 'url_column', 'output.csv') Integration with Other Libraries \u00b6 Using with requests \u00b6 import requests from liburlparser import Url, Host def get_domain_info(url_str): # Parse the URL first to validate it url = Url(url_str) # Make the request response = requests.get(url_str) # Check if there were redirects if response.history: final_url = response.url final_domain = Host.from_url(final_url).domain print(f\"Redirected from {url.domain} to {final_domain}\") return { 'original_domain': url.domain, 'final_domain': Host.from_url(response.url).domain, 'status_code': response.status_code } # Example usage # info = get_domain_info(\"http://github.com\") Using with pandas \u00b6 import pandas as pd from liburlparser import Host def extract_domains(df, url_column): # Create a function to extract domain info def get_domain_info(url): try: info = Host.extract_from_url(url) return pd.Series([ info['domain'], info['suffix'], info['subdomain'] ]) except: return pd.Series(['', '', '']) # Apply the function to the URL column domain_info = df[url_column].apply(get_domain_info) domain_info.columns = ['domain', 'suffix', 'subdomain'] # Join the results back to the original dataframe return pd.concat([df, domain_info], axis=1) # Example usage # df = pd.read_csv('urls.csv') # df_with_domains = extract_domains(df, 'url_column') Handling Edge Cases \u00b6 liburlparser is designed to handle various edge cases: # IP addresses as hosts url = Url(\"https://192.168.1.1/path\") print(url.host) # <Host :'192.168.1.1'> print(url.domain) # (empty string, as IPs don't have domains) # Localhost url = Url(\"http://localhost:8080\") print(url.host) # <Host :'localhost'> print(url.domain) # localhost print(url.suffix) # (empty string) # URLs with unusual ports url = Url(\"https://example.com:65535/path\") print(url.port) # 65535 # URLs with empty paths url = Url(\"https://example.com\") print(url.path) # (empty string) # URLs with query parameters but no path url = Url(\"https://example.com?param=value\") print(url.query) # param=value Custom PSL Rules \u00b6 You can create and use a custom Public Suffix List with additional rules: from liburlparser import psl # Create a custom PSL with additional rules custom_psl = \"\"\" // Standard PSL rules com org net // Custom rules for internal domains internal local dev test \"\"\" # Load the custom PSL psl.load_from_string(custom_psl) # Now parse domains with custom suffixes host = Host(\"example.internal\") print(host.domain) # example print(host.suffix) # internal host = Host(\"server.dev\") print(host.domain) # server print(host.suffix) # dev Thread Safety \u00b6 liburlparser is thread-safe for parsing operations, but PSL loading should be done before spawning threads: import threading from liburlparser import Host, psl import time # Make sure PSL is loaded before creating threads if not psl.is_loaded(): psl.load_from_path(\"/path/to/public_suffix_list.dat\") def parse_hosts(hosts, results, thread_id): for host_str in hosts: try: host = Host(host_str) results.append({ 'thread': thread_id, 'host': host_str, 'domain': host.domain, 'suffix': host.suffix }) except Exception as e: results.append({ 'thread': thread_id, 'host': host_str, 'error': str(e) }) # Example usage def multi_threaded_parsing(all_hosts, num_threads=4): threads = [] results = [] # Split hosts among threads chunk_size = len(all_hosts) // num_threads for i in range(num_threads): start = i * chunk_size end = start + chunk_size if i < num_threads - 1 else len(all_hosts) hosts_chunk = all_hosts[start:end] thread = threading.Thread( target=parse_hosts, args=(hosts_chunk, results, i) ) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() return results # Example usage # hosts = [\"example.com\", \"google.com\", \"github.com\", ...] * 1000 # results = multi_threaded_parsing(hosts, num_threads=8) Memory Management \u00b6 For processing very large datasets with minimal memory usage: from liburlparser import Host import csv def process_urls_streaming(input_file, output_file, batch_size=10000): with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile: reader = csv.reader(infile) writer = csv.writer(outfile) writer.writerow(['url', 'domain', 'suffix', 'subdomain']) batch = [] for i, row in enumerate(reader): if i == 0: # Skip header continue url = row[0] batch.append(url) # Process in batches to avoid memory issues if len(batch) >= batch_size: process_batch(batch, writer) batch = [] # Process remaining URLs if batch: process_batch(batch, writer) def process_batch(urls, writer): for url in urls: try: info = Host.extract_from_url(url) writer.writerow([ url, info['domain'], info['suffix'], info['subdomain'] ]) except Exception: writer.writerow([url, '', '', '']) # Example usage # process_urls_streaming('millions_of_urls.csv', 'domains.csv', batch_size=50000) Advanced URL Parsing \u00b6 For more complex URL parsing needs: from liburlparser import Url import urllib.parse def parse_query_params(url_str): url = Url(url_str) query = url.query # Parse query parameters params = urllib.parse.parse_qs(query) return { 'url': url_str, 'domain': url.domain, 'path': url.path, 'params': params } # Example result = parse_query_params(\"https://example.com/search?q=test&page=1&filter=active\") print(result) # { # 'url': 'https://example.com/search?q=test&page=1&filter=active', # 'domain': 'example', # 'path': '/search', # 'params': {'q': ['test'], 'page': ['1'], 'filter': ['active']} # } Benchmarking \u00b6 You can benchmark liburlparser against other libraries: import time from liburlparser import Host # Try to import other libraries try: import tldextract has_tldextract = True except ImportError: has_tldextract = False try: import publicsuffix2 has_publicsuffix2 = True except ImportError: has_publicsuffix2 = False def benchmark(urls, iterations=3): results = {} # Benchmark liburlparser total_time = 0 for _ in range(iterations): start = time.time() for url in urls: Host.extract_from_url(url) total_time += time.time() - start results['liburlparser'] = total_time / iterations # Benchmark tldextract if available if has_tldextract: total_time = 0 for _ in range(iterations): start = time.time() for url in urls: tldextract.extract(url) total_time += time.time() - start results['tldextract'] = total_time / iterations # Benchmark publicsuffix2 if available if has_publicsuffix2: total_time = 0 for _ in range(iterations): start = time.time() for url in urls: try: publicsuffix2.get_sld(url) except: pass total_time += time.time() - start results['publicsuffix2'] = total_time / iterations return results # Example usage # urls = [\"https://example.com\", \"https://mail.google.com\", ...] * 1000 # results = benchmark(urls) # print(results) Conclusion \u00b6 This advanced guide covers the more complex aspects of using liburlparser in Python. By leveraging these techniques, you can build high-performance applications that efficiently process and analyze URLs and domains. For more information, check out the API Reference or the Examples page.","title":"Advanced Usage"},{"location":"usage/advanced/#advanced-usage","text":"This guide covers more advanced features and techniques for using liburlparser in Python.","title":"Advanced Usage"},{"location":"usage/advanced/#working-with-the-public-suffix-list-psl","text":"liburlparser uses the Public Suffix List to accurately parse domain suffixes. You can interact with the PSL through the global psl object: from liburlparser import psl # Check if PSL is loaded print(psl.is_loaded()) # True (by default) # Get PSL information print(psl.url) # URL of the public suffix list print(psl.filename) # Filename of the public suffix list # Load PSL from a custom path psl.load_from_path(\"/path/to/custom/public_suffix_list.dat\") # Load PSL from a string with open(\"/path/to/custom/public_suffix_list.dat\", \"r\") as f: psl_content = f.read() psl.load_from_string(psl_content)","title":"Working with the Public Suffix List (PSL)"},{"location":"usage/advanced/#ignoring-www-subdomain","text":"You can choose to ignore the \"www\" subdomain when parsing: # Default behavior host = Host(\"www.example.com\") print(host.subdomain) # www print(host.domain) # example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(host.subdomain) # (empty string) print(host.domain) # example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(url.subdomain) # (empty string) print(url.domain) # example","title":"Ignoring \"www\" Subdomain"},{"location":"usage/advanced/#removing-www-from-a-host-string","text":"If you just need to remove \"www.\" from a hostname: from liburlparser import Host host_str = Host.removeWWW(\"www.example.com\") print(host_str) # example.com","title":"Removing \"www\" from a Host String"},{"location":"usage/advanced/#handling-complex-domains","text":"liburlparser can handle complex domain structures, including multi-part suffixes: # Standard TLDs host = Host(\"example.com\") print(host.domain) # example print(host.suffix) # com # Country-specific TLDs host = Host(\"example.co.uk\") print(host.domain) # example print(host.suffix) # co.uk # Multi-level subdomains host = Host(\"a.b.c.example.com\") print(host.subdomain) # a.b.c print(host.domain) # example print(host.suffix) # com","title":"Handling Complex Domains"},{"location":"usage/advanced/#compatibility-with-other-libraries","text":"If you're migrating from pydomainextractor, liburlparser provides compatible methods: # pydomainextractor style: import pydomainextractor extractor = pydomainextractor.DomainExtractor() result = extractor.extract(\"mail.google.com\") result = extractor.extract_from_url(\"https://mail.google.com/about\") # liburlparser equivalent: from liburlparser import Host result = Host.extract(\"mail.google.com\") result = Host.extract_from_url(\"https://mail.google.com/about\")","title":"Compatibility with Other Libraries"},{"location":"usage/advanced/#error-handling","text":"liburlparser is designed to be robust and handle various edge cases: # Empty or invalid URLs try: url = Url(\"\") except Exception as e: print(f\"Error: {e}\") # URLs without protocols url = Url(\"example.com\") print(url.protocol) # (empty string) print(url.domain) # example # URLs with unusual formats url = Url(\"https://user:pass@example.com:8080/path?query=value#fragment\") print(url.userinfo) # user:pass print(url.port) # 8080 print(url.query) # query=value","title":"Error Handling"},{"location":"usage/advanced/#performance-optimization","text":"For high-performance applications, consider these techniques: Use Host.from_url() instead of creating a Url object when you only need host information: # Slower (creates full Url object) url = Url(\"https://example.com/path\") host = url.host # Faster (directly extracts host) host = Host.from_url(\"https://example.com/path\") Use Url.extract_host() when you only need the host string: # Fastest (just extracts host string without parsing) host_str = Url.extract_host(\"https://example.com/path\") Use Host.extract() or Host.extract_from_url() when you only need domain components: # Fast (returns just the components you need) components = Host.extract(\"example.com\") components = Host.extract_from_url(\"https://example.com/path\")","title":"Performance Optimization"},{"location":"usage/advanced/#working-with-international-domain-names-idns","text":"liburlparser has built-in support for International Domain Names (IDNs): # Parse IDNs directly host = Host(\"\u4f8b\u5b50.\u6d4b\u8bd5\") print(host.domain) # \u4f8b\u5b50 print(host.suffix) # \u6d4b\u8bd5 # IDNs in URLs url = Url(\"https://\u4f8b\u5b50.\u6d4b\u8bd5/path\") print(url.domain) # \u4f8b\u5b50 print(url.suffix) # \u6d4b\u8bd5 # Mixed IDN and ASCII host = Host(\"subdomain.\u4f8b\u5b50.\u6d4b\u8bd5\") print(host.subdomain) # subdomain print(host.domain) # \u4f8b\u5b50 print(host.suffix) # \u6d4b\u8bd5","title":"Working with International Domain Names (IDNs)"},{"location":"usage/advanced/#batch-processing","text":"For processing large numbers of URLs efficiently: from liburlparser import Host import csv def extract_domains_from_csv(csv_file, url_column, output_file): with open(csv_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile: reader = csv.DictReader(infile) fieldnames = list(reader.fieldnames) + ['domain', 'suffix', 'subdomain'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: url = row[url_column] try: # Use the fast extract_from_url method domain_info = Host.extract_from_url(url) row['domain'] = domain_info['domain'] row['suffix'] = domain_info['suffix'] row['subdomain'] = domain_info['subdomain'] except Exception: row['domain'] = '' row['suffix'] = '' row['subdomain'] = '' writer.writerow(row) # Example usage # extract_domains_from_csv('urls.csv', 'url_column', 'output.csv')","title":"Batch Processing"},{"location":"usage/advanced/#integration-with-other-libraries","text":"","title":"Integration with Other Libraries"},{"location":"usage/advanced/#using-with-requests","text":"import requests from liburlparser import Url, Host def get_domain_info(url_str): # Parse the URL first to validate it url = Url(url_str) # Make the request response = requests.get(url_str) # Check if there were redirects if response.history: final_url = response.url final_domain = Host.from_url(final_url).domain print(f\"Redirected from {url.domain} to {final_domain}\") return { 'original_domain': url.domain, 'final_domain': Host.from_url(response.url).domain, 'status_code': response.status_code } # Example usage # info = get_domain_info(\"http://github.com\")","title":"Using with requests"},{"location":"usage/advanced/#using-with-pandas","text":"import pandas as pd from liburlparser import Host def extract_domains(df, url_column): # Create a function to extract domain info def get_domain_info(url): try: info = Host.extract_from_url(url) return pd.Series([ info['domain'], info['suffix'], info['subdomain'] ]) except: return pd.Series(['', '', '']) # Apply the function to the URL column domain_info = df[url_column].apply(get_domain_info) domain_info.columns = ['domain', 'suffix', 'subdomain'] # Join the results back to the original dataframe return pd.concat([df, domain_info], axis=1) # Example usage # df = pd.read_csv('urls.csv') # df_with_domains = extract_domains(df, 'url_column')","title":"Using with pandas"},{"location":"usage/advanced/#handling-edge-cases","text":"liburlparser is designed to handle various edge cases: # IP addresses as hosts url = Url(\"https://192.168.1.1/path\") print(url.host) # <Host :'192.168.1.1'> print(url.domain) # (empty string, as IPs don't have domains) # Localhost url = Url(\"http://localhost:8080\") print(url.host) # <Host :'localhost'> print(url.domain) # localhost print(url.suffix) # (empty string) # URLs with unusual ports url = Url(\"https://example.com:65535/path\") print(url.port) # 65535 # URLs with empty paths url = Url(\"https://example.com\") print(url.path) # (empty string) # URLs with query parameters but no path url = Url(\"https://example.com?param=value\") print(url.query) # param=value","title":"Handling Edge Cases"},{"location":"usage/advanced/#custom-psl-rules","text":"You can create and use a custom Public Suffix List with additional rules: from liburlparser import psl # Create a custom PSL with additional rules custom_psl = \"\"\" // Standard PSL rules com org net // Custom rules for internal domains internal local dev test \"\"\" # Load the custom PSL psl.load_from_string(custom_psl) # Now parse domains with custom suffixes host = Host(\"example.internal\") print(host.domain) # example print(host.suffix) # internal host = Host(\"server.dev\") print(host.domain) # server print(host.suffix) # dev","title":"Custom PSL Rules"},{"location":"usage/advanced/#thread-safety","text":"liburlparser is thread-safe for parsing operations, but PSL loading should be done before spawning threads: import threading from liburlparser import Host, psl import time # Make sure PSL is loaded before creating threads if not psl.is_loaded(): psl.load_from_path(\"/path/to/public_suffix_list.dat\") def parse_hosts(hosts, results, thread_id): for host_str in hosts: try: host = Host(host_str) results.append({ 'thread': thread_id, 'host': host_str, 'domain': host.domain, 'suffix': host.suffix }) except Exception as e: results.append({ 'thread': thread_id, 'host': host_str, 'error': str(e) }) # Example usage def multi_threaded_parsing(all_hosts, num_threads=4): threads = [] results = [] # Split hosts among threads chunk_size = len(all_hosts) // num_threads for i in range(num_threads): start = i * chunk_size end = start + chunk_size if i < num_threads - 1 else len(all_hosts) hosts_chunk = all_hosts[start:end] thread = threading.Thread( target=parse_hosts, args=(hosts_chunk, results, i) ) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() return results # Example usage # hosts = [\"example.com\", \"google.com\", \"github.com\", ...] * 1000 # results = multi_threaded_parsing(hosts, num_threads=8)","title":"Thread Safety"},{"location":"usage/advanced/#memory-management","text":"For processing very large datasets with minimal memory usage: from liburlparser import Host import csv def process_urls_streaming(input_file, output_file, batch_size=10000): with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile: reader = csv.reader(infile) writer = csv.writer(outfile) writer.writerow(['url', 'domain', 'suffix', 'subdomain']) batch = [] for i, row in enumerate(reader): if i == 0: # Skip header continue url = row[0] batch.append(url) # Process in batches to avoid memory issues if len(batch) >= batch_size: process_batch(batch, writer) batch = [] # Process remaining URLs if batch: process_batch(batch, writer) def process_batch(urls, writer): for url in urls: try: info = Host.extract_from_url(url) writer.writerow([ url, info['domain'], info['suffix'], info['subdomain'] ]) except Exception: writer.writerow([url, '', '', '']) # Example usage # process_urls_streaming('millions_of_urls.csv', 'domains.csv', batch_size=50000)","title":"Memory Management"},{"location":"usage/advanced/#advanced-url-parsing","text":"For more complex URL parsing needs: from liburlparser import Url import urllib.parse def parse_query_params(url_str): url = Url(url_str) query = url.query # Parse query parameters params = urllib.parse.parse_qs(query) return { 'url': url_str, 'domain': url.domain, 'path': url.path, 'params': params } # Example result = parse_query_params(\"https://example.com/search?q=test&page=1&filter=active\") print(result) # { # 'url': 'https://example.com/search?q=test&page=1&filter=active', # 'domain': 'example', # 'path': '/search', # 'params': {'q': ['test'], 'page': ['1'], 'filter': ['active']} # }","title":"Advanced URL Parsing"},{"location":"usage/advanced/#benchmarking","text":"You can benchmark liburlparser against other libraries: import time from liburlparser import Host # Try to import other libraries try: import tldextract has_tldextract = True except ImportError: has_tldextract = False try: import publicsuffix2 has_publicsuffix2 = True except ImportError: has_publicsuffix2 = False def benchmark(urls, iterations=3): results = {} # Benchmark liburlparser total_time = 0 for _ in range(iterations): start = time.time() for url in urls: Host.extract_from_url(url) total_time += time.time() - start results['liburlparser'] = total_time / iterations # Benchmark tldextract if available if has_tldextract: total_time = 0 for _ in range(iterations): start = time.time() for url in urls: tldextract.extract(url) total_time += time.time() - start results['tldextract'] = total_time / iterations # Benchmark publicsuffix2 if available if has_publicsuffix2: total_time = 0 for _ in range(iterations): start = time.time() for url in urls: try: publicsuffix2.get_sld(url) except: pass total_time += time.time() - start results['publicsuffix2'] = total_time / iterations return results # Example usage # urls = [\"https://example.com\", \"https://mail.google.com\", ...] * 1000 # results = benchmark(urls) # print(results)","title":"Benchmarking"},{"location":"usage/advanced/#conclusion","text":"This advanced guide covers the more complex aspects of using liburlparser in Python. By leveraging these techniques, you can build high-performance applications that efficiently process and analyze URLs and domains. For more information, check out the API Reference or the Examples page.","title":"Conclusion"},{"location":"usage/basic/","text":"Basic Usage \u00b6 This guide covers the fundamental operations of liburlparser in Python. Importing the Library \u00b6 from liburlparser import Url, Host Parsing URLs \u00b6 The Url class is used to parse complete URLs: # Create a URL object url = Url(\"https://mail.google.com/about?q=test#section\") # Access URL components print(url) # <Url :'https://mail.google.com/about?q=test#section'> print(url.protocol) # https print(url.host) # <Host :'mail.google.com'> print(url.subdomain) # mail print(url.domain) # google print(url.domain_name) # google print(url.suffix) # com print(url.port) # 0 (default) print(url.query) # q=test print(url.fragment) # section Parsing Hosts \u00b6 The Host class is used to parse hostnames: # Create a Host object directly host = Host(\"mail.google.com\") # Access host components print(host) # <Host :'mail.google.com'> print(host.subdomain) # mail print(host.domain) # google print(host.domain_name) # google print(host.suffix) # com Getting Host from URL \u00b6 There are multiple ways to extract host information from a URL: # Method 1: Get the host from a URL object url = Url(\"https://mail.google.com/about\") host = url.host # Method 2: Use the Host.from_url static method host = Host.from_url(\"https://mail.google.com/about\") # Method 3: Just extract the host string (fastest) host_str = Url.extract_host(\"https://mail.google.com/about\") print(host_str) # mail.google.com Converting to Dictionary or JSON \u00b6 Both Url and Host objects can be converted to dictionaries or JSON strings: # URL to dictionary url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(url_dict) # Output: {'str': 'https://mail.google.com/about?q=test#section', 'protocol': 'https', 'userinfo': '', 'host': {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'}, 'port': 0, 'query': 'q=test', 'fragment': 'section'} # URL to JSON url_json = url.to_json() print(url_json) # Output: {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"} # Host to dictionary host = Host(\"mail.google.com\") host_dict = host.to_dict() print(host_dict) # Output: {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'} # Host to JSON host_json = host.to_json() print(host_json) # Output: {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"} Quick Domain Extraction \u00b6 If you only need the domain components without creating full objects: # From a host string result = Host.extract(\"mail.google.com\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} # From a URL string result = Host.extract_from_url(\"https://mail.google.com/about\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} Ignoring \"www\" Subdomain \u00b6 You can choose to ignore the \"www\" subdomain when parsing: # Default behavior host = Host(\"www.example.com\") print(host.subdomain) # www print(host.domain) # example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(host.subdomain) # (empty string) print(host.domain) # example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(url.subdomain) # (empty string) print(url.domain) # example Removing \"www\" from a Host String \u00b6 host_str = Host.removeWWW(\"www.example.com\") print(host_str) # example.com Complete Example \u00b6 Here's a complete example that demonstrates parsing a URL and accessing its components: from liburlparser import Url, Host def analyze_url(url_str): # Parse the URL url = Url(url_str) # Print URL components print(f\"Full URL: {url}\") print(f\"Protocol: {url.protocol}\") print(f\"Host: {url.host}\") print(f\"Subdomain: {url.subdomain}\") print(f\"Domain: {url.domain}\") print(f\"Suffix: {url.suffix}\") print(f\"Port: {url.port}\") print(f\"Query: {url.query}\") print(f\"Fragment: {url.fragment}\") # Convert to JSON print(f\"JSON: {url.to_json()}\") # Test with a sample URL analyze_url(\"https://mail.google.com/about?q=test#section\")","title":"Basic Usage"},{"location":"usage/basic/#basic-usage","text":"This guide covers the fundamental operations of liburlparser in Python.","title":"Basic Usage"},{"location":"usage/basic/#importing-the-library","text":"from liburlparser import Url, Host","title":"Importing the Library"},{"location":"usage/basic/#parsing-urls","text":"The Url class is used to parse complete URLs: # Create a URL object url = Url(\"https://mail.google.com/about?q=test#section\") # Access URL components print(url) # <Url :'https://mail.google.com/about?q=test#section'> print(url.protocol) # https print(url.host) # <Host :'mail.google.com'> print(url.subdomain) # mail print(url.domain) # google print(url.domain_name) # google print(url.suffix) # com print(url.port) # 0 (default) print(url.query) # q=test print(url.fragment) # section","title":"Parsing URLs"},{"location":"usage/basic/#parsing-hosts","text":"The Host class is used to parse hostnames: # Create a Host object directly host = Host(\"mail.google.com\") # Access host components print(host) # <Host :'mail.google.com'> print(host.subdomain) # mail print(host.domain) # google print(host.domain_name) # google print(host.suffix) # com","title":"Parsing Hosts"},{"location":"usage/basic/#getting-host-from-url","text":"There are multiple ways to extract host information from a URL: # Method 1: Get the host from a URL object url = Url(\"https://mail.google.com/about\") host = url.host # Method 2: Use the Host.from_url static method host = Host.from_url(\"https://mail.google.com/about\") # Method 3: Just extract the host string (fastest) host_str = Url.extract_host(\"https://mail.google.com/about\") print(host_str) # mail.google.com","title":"Getting Host from URL"},{"location":"usage/basic/#converting-to-dictionary-or-json","text":"Both Url and Host objects can be converted to dictionaries or JSON strings: # URL to dictionary url = Url(\"https://mail.google.com/about?q=test#section\") url_dict = url.to_dict() print(url_dict) # Output: {'str': 'https://mail.google.com/about?q=test#section', 'protocol': 'https', 'userinfo': '', 'host': {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'}, 'port': 0, 'query': 'q=test', 'fragment': 'section'} # URL to JSON url_json = url.to_json() print(url_json) # Output: {\"str\": \"https://mail.google.com/about?q=test#section\", \"protocol\": \"https\", \"userinfo\": \"\", \"host\": {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}, \"port\": 0, \"query\": \"q=test\", \"fragment\": \"section\"} # Host to dictionary host = Host(\"mail.google.com\") host_dict = host.to_dict() print(host_dict) # Output: {'str': 'mail.google.com', 'subdomain': 'mail', 'domain': 'google', 'domain_name': 'google', 'suffix': 'com'} # Host to JSON host_json = host.to_json() print(host_json) # Output: {\"str\": \"mail.google.com\", \"subdomain\": \"mail\", \"domain\": \"google\", \"domain_name\": \"google\", \"suffix\": \"com\"}","title":"Converting to Dictionary or JSON"},{"location":"usage/basic/#quick-domain-extraction","text":"If you only need the domain components without creating full objects: # From a host string result = Host.extract(\"mail.google.com\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'} # From a URL string result = Host.extract_from_url(\"https://mail.google.com/about\") print(result) # {'suffix': 'com', 'domain': 'google', 'subdomain': 'mail'}","title":"Quick Domain Extraction"},{"location":"usage/basic/#ignoring-www-subdomain","text":"You can choose to ignore the \"www\" subdomain when parsing: # Default behavior host = Host(\"www.example.com\") print(host.subdomain) # www print(host.domain) # example # Ignore www host = Host(\"www.example.com\", ignore_www=True) print(host.subdomain) # (empty string) print(host.domain) # example # Same for URLs url = Url(\"https://www.example.com/about\", ignore_www=True) print(url.subdomain) # (empty string) print(url.domain) # example","title":"Ignoring \"www\" Subdomain"},{"location":"usage/basic/#removing-www-from-a-host-string","text":"host_str = Host.removeWWW(\"www.example.com\") print(host_str) # example.com","title":"Removing \"www\" from a Host String"},{"location":"usage/basic/#complete-example","text":"Here's a complete example that demonstrates parsing a URL and accessing its components: from liburlparser import Url, Host def analyze_url(url_str): # Parse the URL url = Url(url_str) # Print URL components print(f\"Full URL: {url}\") print(f\"Protocol: {url.protocol}\") print(f\"Host: {url.host}\") print(f\"Subdomain: {url.subdomain}\") print(f\"Domain: {url.domain}\") print(f\"Suffix: {url.suffix}\") print(f\"Port: {url.port}\") print(f\"Query: {url.query}\") print(f\"Fragment: {url.fragment}\") # Convert to JSON print(f\"JSON: {url.to_json()}\") # Test with a sample URL analyze_url(\"https://mail.google.com/about?q=test#section\")","title":"Complete Example"},{"location":"usage/cli/","text":"Command Line Usage \u00b6 liburlparser provides a command-line interface through its Python module, allowing you to parse URLs and hosts directly from the terminal. Basic Commands \u00b6 Getting Help \u00b6 To see all available options: python -m liburlparser --help Output:","title":"Command Line"},{"location":"usage/cli/#command-line-usage","text":"liburlparser provides a command-line interface through its Python module, allowing you to parse URLs and hosts directly from the terminal.","title":"Command Line Usage"},{"location":"usage/cli/#basic-commands","text":"","title":"Basic Commands"},{"location":"usage/cli/#getting-help","text":"To see all available options: python -m liburlparser --help Output:","title":"Getting Help"}]}